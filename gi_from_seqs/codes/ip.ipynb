{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import socket\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "floyd_flag = True\n",
    "if floyd_flag:\n",
    "    import process\n",
    "else:\n",
    "    import gi_from_seqs.codes.process as process\n",
    "\n",
    "\n",
    "# load equal length input data from file\n",
    "train_data,dev_data,test_data, input_size = process.load_data(floyd_flag)\n",
    "\n",
    "def tensor2variable(tensor, target_flag):\n",
    "    if target_flag:\n",
    "        result = Variable(torch.FloatTensor(tensor))  # target is float type\n",
    "    else:\n",
    "        result = Variable(torch.LongTensor(tensor))  # MAXLEN * 1 \"column vector\n",
    "    # move to gpu\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "def get_batch(batch_size, data, idx=0):\n",
    "    # get a batch of data from index == idx\n",
    "    input_seqs = []\n",
    "    target_scores = []\n",
    "\n",
    "    # choose pair from index\n",
    "    for i in range(batch_size):\n",
    "        pair = data[idx]\n",
    "        input_seqs.append(pair[0])\n",
    "        target_scores.append(pair[1])\n",
    "        idx += 1\n",
    "        # print(input_seqs,target_scores)\n",
    "\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors,\n",
    "    # transpose into (max_len x batch_size)\n",
    "    input_var = tensor2variable(input_seqs, False).transpose(0, 1)\n",
    "    target_var = tensor2variable(target_scores, True)#.transpose(0)\n",
    "    # print(input_var, target_var)\n",
    "\n",
    "    return input_var,target_var\n",
    "\n",
    "# define encoder module\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size,embed_size, hidden_size, n_layers=2, dropout=0.2,is_bidirectional=True):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=is_bidirectional)\n",
    "\n",
    "    def forward(self, input_seqs, hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        if self.is_bidirectional:\n",
    "            outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n",
    "        else:\n",
    "            outputs = outputs[:, :, :self.hidden_size]\n",
    "\n",
    "        return outputs,hidden\n",
    "\n",
    "# build attention module\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn,self).__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "        if use_cuda:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # for each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                # print(\"hidden[b,:]:\",hidden[b,:],\"encoder\", encoder_outputs[i,b])\n",
    "                attn_energies[b, i] = self.score(hidden[b,:], encoder_outputs[i,b])\n",
    "                # hidden[b,:] (size=hidden_size)  ; encoder_outputs[i,b] (size = hidden_size)\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to  1 x B X S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "\n",
    "    def score(self,x, y):\n",
    "        if self.method == 'dot':\n",
    "            energy = torch.dot(x,y)\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(y)\n",
    "            energy = x.dot(energy)\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((x, y),1))\n",
    "            energy = self.self.v.dot(energy)\n",
    "            return energy\n",
    "\n",
    "# dense layer predictor model\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, method, hidden_size, output_size=1, n_layers=3):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.method = method\n",
    "\n",
    "        # Define layers\n",
    "        self.attn = Attn(method, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_last_hidden, encoder_outputs):\n",
    "        # print(\"last_hidden[-1]\",last_hidden[-1]) # 3x6\n",
    "        attn_weights = self.attn(encoder_last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1)) # B X 1 X hidden_size\n",
    "        context = context.transpose(0,1) # 1 x B x  hidden_size\n",
    "        output = context.squeeze(0) # BxN\n",
    "        output = self.out(output) # B x hidden_size\n",
    "        return output, attn_weights #, context     ### uncomment when debugging\n",
    "\n",
    "\n",
    "class VPmodel(nn.Module):\n",
    "    def __init__(self, method, input_size, embed_size, hidden_size, output_size=1,dropout = 0.1,\n",
    "                 encoder_layers = 2, bidirectional=True):\n",
    "        super(VPmodel, self).__init__()\n",
    "\n",
    "        # Define parameters\n",
    "        self.method = method\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.encoder = EncoderRNN(input_size, embed_size,hidden_size,encoder_layers, dropout,bidirectional)\n",
    "        self.out = Predictor(method,hidden_size)\n",
    "    def forward(self,input_seqs, hidden=None):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seqs, None)\n",
    "        # Prepare predictor input\n",
    "        predictor_input = encoder_hidden[:1]  # 1 x batch_size x hidden_size\n",
    "        # predict\n",
    "        predictor_output, predictor_attn_weights = self.out(predictor_input, encoder_outputs)\n",
    "        return predictor_output,predictor_attn_weights\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n",
    "\n",
    "def get_correlation(truth, pred):\n",
    "    assert len(truth) == len(pred)\n",
    "    correlation = scipy.stats.pearsonr(truth,pred)\n",
    "    return correlation\n",
    "\n",
    "\n",
    "# Configure training/optimization\n",
    "attnModel = 'dot'\n",
    "embedSize = 128\n",
    "hiddenSize = 64\n",
    "dropout = 0.4\n",
    "batchSize = 100\n",
    "inputSize = input_size\n",
    "outputSize = 1\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "# initialize model\n",
    "interaction_predictor = VPmodel(attnModel,inputSize,embedSize,hiddenSize,outputSize,dropout=dropout)\n",
    "# initialize optimizers and criterion\n",
    "interaction_predictor_optimizer = optim.Adam(interaction_predictor.parameters())\n",
    "\n",
    "# Move models to GPU\n",
    "if use_cuda:\n",
    "    interaction_predictor.cuda()\n",
    "\n",
    "# keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "print_loss_total = 0 # reset every print_every\n",
    "plot_loss_total = 0 # reset every plot_every\n",
    "\n",
    "\n",
    "# defining a training iteration\n",
    "def train(input_batches, target_batches,interaction_predictor, interaction_predictor_optimizer):\n",
    "    # zero gradients of the optimizers\n",
    "    interaction_predictor_optimizer.zero_grad()\n",
    "    # run words through encoder\n",
    "    predictor_output,predictor_attn_weights = interaction_predictor(input_batches)\n",
    "    # print(\"predicted:\", predictor_output,\"\\n\", \"target:\", target_batches)\n",
    "    # Loss calculation and back-propagation\n",
    "    loss = torch.nn.MSELoss()\n",
    "    output = loss(predictor_output,target_batches)  # output, target\n",
    "    output.backward()\n",
    "    interaction_predictor_optimizer.step() # update parameters with optimizers\n",
    "\n",
    "    return output.data[0]\n",
    "\n",
    "\n",
    "def evaluate(batch_size, data):\n",
    "    batch_i = 0\n",
    "    num_of_records = len(data)\n",
    "    num_batches = int(num_of_records // batch_size)\n",
    "\n",
    "    predicted_all = []\n",
    "    true_all = []\n",
    "    attn_all = []\n",
    "\n",
    "    while batch_i <  num_batches:\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        input_batches, target_batches = get_batch(batch_size, data,idx=start_i)\n",
    "\n",
    "        interaction_predictor.train(False)\n",
    "\n",
    "        # run through prediction model\n",
    "        predictor_output, predictor_attn = interaction_predictor(input_batches)\n",
    "        # print(\"predictor_output:\", predictor_output)\n",
    "\n",
    "        loss = torch.nn.MSELoss()\n",
    "        output = loss(predictor_output, target_batches)\n",
    "        print('eval_loss', output.data[0])\n",
    "\n",
    "        if use_cuda:\n",
    "            predicted = predictor_output.topk(1)[1].data.cpu().numpy().tolist()\n",
    "            true = target_batches.transpose(0, 1).contiguous().data.cpu().numpy().tolist()\n",
    "            attn = predictor_attn.squeeze(1).data.cpu().numpy().tolist()\n",
    "        else:\n",
    "            predicted = predictor_output\n",
    "            true = target_batches.contiguous().data.numpy().tolist()\n",
    "            attn = predictor_attn.squeeze(1).data.numpy().tolist()\n",
    "\n",
    "        predicted_all.extend(predicted)\n",
    "        true_all.extend(true)\n",
    "        attn_all.extend(attn)\n",
    "        batch_i += 1\n",
    "\n",
    "    interaction_predictor.train(True)\n",
    "    return predicted_all, true_all,attn_all\n",
    "\n",
    "def train_minibatch(train_data,eval_data,batch_size,print_loss_total=0, plot_loss_total=0):\n",
    "    batch_i = 0  #initialize batch index\n",
    "    num_of_records = len(train_data)\n",
    "    num_batches = int(num_of_records//batch_size)\n",
    "    evaluate_every = 10\n",
    "    # shuffle the data at each epoch\n",
    "    # random.shuffle(train_data)\n",
    "\n",
    "    while batch_i < num_batches:\n",
    "        print(\"batch:\", batch_i, \"total_batches:\", num_batches, int(batch_i * 100 // num_batches), \"%\")\n",
    "        start_i = batch_i * batch_size\n",
    "        input_batches, target_batches = get_batch(batch_size,train_data, idx=start_i) # get training data for this cycle\n",
    "\n",
    "        # run the train function\n",
    "        loss = train(input_batches, target_batches,interaction_predictor,interaction_predictor_optimizer)\n",
    "\n",
    "        # keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if (batch_i+1) % print_every == 0:\n",
    "            print_loss_avg = print_loss_total/print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary ='Time:%s, Batch:(%d %d%%), Avg_loss:%.4f' %\\\n",
    "                           (time_since(start, batch_i / num_batches),\n",
    "                            batch_i,batch_i / num_batches * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "\n",
    "        if (batch_i+1) % evaluate_every == 0:   # evaluate on dev set after each epoch\n",
    "            predicted, true, attn= evaluate(batch_size, eval_data)\n",
    "            # print(\"predicted:\", predicted, len(predicted),\"\\n\",\"true:\",true,len(true))\n",
    "            acc = get_correlation(true,predicted)\n",
    "            print(\"accuracy: \", acc)\n",
    "\n",
    "        batch_i += 1  # update batch index\n",
    "\n",
    "\n",
    "def train_epochs(num_epochs):\n",
    "    for i in range(num_epochs):\n",
    "        print(\"Epoch: %d/%d\"%(i,num_epochs))\n",
    "        train_minibatch(train_data,dev_data,64)\n",
    "\n",
    "# # training\n",
    "train_epochs(1)\n",
    "# # test on test set\n",
    "# predicted, true, attn= evaluate(100, data=test_data)\n",
    "# acc = get_accuracy(true, predicted)\n",
    "# print(\"accuracy: \", acc)\n",
    "#\n",
    "# ## save model and output for further analysis\n",
    "# with open('/output/output.pickle', 'wb') as f:\n",
    "#     pickle.dump([predicted, true, attn], f)\n",
    "# with open('/output/test_data.pickle', 'wb') as d:\n",
    "#     pickle.dump(test_data, d)\n",
    "# torch.save(interaction_predictor.state_dict(), '/output/vp.dat')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
